{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "t_path = \"../../pyths/data/nba_1.txt\"\n",
    "m_space = re.compile(r'\\s+')\n",
    "point = re.compile(r'\\W+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text(t_path):\n",
    "    whole_text = []\n",
    "    whole_entity = {}\n",
    "    vocab = set()\n",
    "    m_space = re.compile(r'\\s+')\n",
    "    point = re.compile(r'\\W+')\n",
    "    with open(t_path, 'r', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip().replace(u'\\xa0', u' ').replace(u\"\\'\", u' ').replace(u'\\.', u' ')\n",
    "            text, entity = line.lower().split('\\t')\n",
    "            text = point.sub(' ', text)\n",
    "            for token in entity.split(','):\n",
    "                index = whole_entity.setdefault(token, len(whole_entity))\n",
    "            t_l = []\n",
    "            whole_text.append(t_l)\n",
    "            for token in text.strip('\\t').split(' '):\n",
    "                if token not in whole_entity:\n",
    "                    vocab.add(token)\n",
    "                    t_l.append(token)\n",
    "    indptr = [0]\n",
    "    indice = []\n",
    "    data = []\n",
    "    vocab = {}\n",
    "    for d in whole_text:\n",
    "        #print(d)\n",
    "        for token in d:\n",
    "            index = vocab.setdefault(token, len(vocab))\n",
    "            #print(token, index)\n",
    "            indice.append(index)        \n",
    "            data.append(1)\n",
    "        indptr.append(len(indice))\n",
    "\n",
    "    bow = csr_matrix((data, indice, indptr), dtype=int).toarray()\n",
    "    \n",
    "    return whole_text, whole_entity, bow, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_text, whole_entity, bow, vocab = read_text(t_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration param\n",
    "T_marker = 1 # time period가 10까지 간다고 가정\n",
    "N_story = 5 # storyline은 5까지 간다\n",
    "N_topic = 8\n",
    "N_docs = len(whole_text)\n",
    "N_Voca = len(vocab)\n",
    "N_NER = len(whole_entity)\n",
    "N_epoch = 3 # 앞선 3개 epoch의 정보만을 참조한다\n",
    "N_token = np.zeros([N_docs], dtype='uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolutionary matrix\n",
    "# storyline s and topic z\n",
    "sig_sz = np.zeros([N_story, N_topic, N_Voca, N_epoch])\n",
    "tau_s = np.zeros([N_story, N_topic, N_epoch])\n",
    "v_s = np.zeros([N_story, N_NER, N_epoch])\n",
    "mu_m = np.zeros([N_epoch])\n",
    "for m in range(N_epoch):\n",
    "    mu_m[m] = np.exp(-0.5*m)\n",
    " \n",
    " \n",
    "pi = np.zeros([T_marker, N_story])\n",
    "theta = np.zeros([T_marker, N_story, N_topic])\n",
    "omega = np.zeros([T_marker, N_story, N_NER])\n",
    "phi = np.zeros([T_marker, N_story, N_topic, N_Voca])\n",
    "s_t = np.zeros([N_story])\n",
    "e = np.zeros([T_marker, N_docs, N_NER])\n",
    " \n",
    " \n",
    "# hyperparameter\n",
    "gamma = np.array([0.1] * N_story)\n",
    "alpha = np.ones([N_story, N_topic]) * 0.1\n",
    "eps = np.array([0.01] * N_NER)\n",
    "beta = np.ones([N_story, N_topic, N_Voca]) * 0.01\n",
    " \n",
    "n_j = np.zeros([N_docs, N_story], dtype='uint32')\n",
    "n_jk = np.zeros([N_docs, N_story, N_topic], dtype='uint32')\n",
    "n_jkv = np.zeros([N_docs, N_story, N_topic, N_Voca], dtype='uint32')\n",
    "s_indicator = np.zeros([N_docs])\n",
    "s_t = np.zeros([N_story])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize latent variable\n",
    "z_agg = []\n",
    "w_agg = []\n",
    "std_agg_by_d = []\n",
    "for d_id, doc in enumerate(whole_text): #print(np.random.multinomial(1, pi[time_tick, :]).argmax())        \n",
    "    N_token[d_id] = len(doc) # 문서 당 길이\n",
    "    z_d = np.zeros([N_token[d_id]], dtype=np.uint32)\n",
    "    w_d = np.zeros([N_token[d_id]], dtype=np.uint32)\n",
    "    #s_indicator[d_id] = np.random.multinomial(1, pi[t]).argmax()\n",
    "    s_indicator[d_id] = np.random.randint(N_story)\n",
    "    #print(int(s_indicator[d_id]))\n",
    "    std_agg = []\n",
    "    for w_pos, token in enumerate(doc):\n",
    "        z_d[w_pos] = np.random.randint(N_topic)\n",
    "        w_d[w_pos] = vocab[token]\n",
    "        n_jk[d_id, int(s_indicator[d_id]), int(z_d[w_pos])] += 1\n",
    "        n_jkv[d_id, int(s_indicator[d_id]), int(z_d[w_pos]), int(w_d[w_pos])] += 1\n",
    "        story_topic_word_d = (s_indicator[d_id], z_d[w_pos], w_d[w_pos])\n",
    "        std_agg.append(story_topic_word_d)\n",
    "    z_agg.append(z_d)\n",
    "    w_agg.append(w_d)\n",
    "    std_agg_by_d.append(std_agg)\n",
    "    n_j[d_id, int(s_indicator[d_id])] += N_token[d_id]\n",
    "\n",
    "for s_id in s_indicator:\n",
    "    s_t[int(s_id)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term = np.zeros([N_docs, N_story])\n",
    " \n",
    "alpha = np.ones([N_story, N_topic]) * 0.1\n",
    "beta = np.ones([N_story, N_topic, N_Voca]) * 0.01\n",
    "nume_3 = np.ones([N_docs, N_story, N_topic], dtype='float64')\n",
    "denom_3 = np.ones([N_docs, N_story], dtype='float64')\n",
    "nume_4 = np.ones([N_docs, N_story, N_topic, N_Voca], dtype='float64')\n",
    "denom_4 = np.ones([N_docs, N_story, N_topic], dtype='float64')\n",
    " \n",
    "third_term = np.ones([N_docs, N_story, N_topic], dtype='float')\n",
    "log_third_term = np.ones([N_docs, N_story], dtype='float64')\n",
    " \n",
    "fourth_term = np.ones([N_docs, N_story, N_topic, N_Voca], dtype='float64')\n",
    "fourth_term_prod = np.ones([N_docs, N_story, N_topic], dtype='float64')\n",
    " \n",
    "N_j = np.zeros([N_docs, N_story], dtype='uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, doc in enumerate(whole_text):\n",
    "    for s in range(N_story):\n",
    "        N_j[d, s] = s_t[s] - 1 if s_indicator[d] == s else s_t[s]\n",
    "    \n",
    "    first_term = (N_j + gamma) / ((N_docs - 1) + N_story * gamma)\n",
    "    s_indicator[d] = np.random.multinomial(1, first_term[d]).argmax()\n",
    "    ## 여기까지 함\n",
    "    np.sum(n_j, 0) - n_j[d] + np.sum(alpha, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  2.,  0.,  3.,  0.,  2.,  0.,  3.,  0.,  0.,  1.,  3.,  4.,\n",
       "        1.,  0.,  3.,  1.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_j[d, int(s_indicator[d])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = np.ones([N_story, N_topic]) * 0.1\n",
    "beta = np.ones([N_story, N_topic, N_Voca]) * 0.01\n",
    "nume_3 = np.ones([N_docs, N_story, N_topic], dtype='float64')\n",
    "denom_3 = np.ones([N_docs, N_story], dtype='float64')\n",
    "nume_4 = np.ones([N_docs, N_story, N_topic, N_Voca], dtype='float64')\n",
    "denom_4 = np.ones([N_docs, N_story, N_topic], dtype='float64')\n",
    " \n",
    "third_term = np.ones([N_docs, N_story, N_topic], dtype='float')\n",
    "log_third_term = np.ones([N_docs, N_story], dtype='float64')\n",
    " \n",
    "fourth_term = np.ones([N_docs, N_story, N_topic, N_Voca], dtype='float64')\n",
    "fourth_term_prod = np.ones([N_docs, N_story, N_topic], dtype='float64')\n",
    " \n",
    "N_j = np.zeros([N_docs, N_story], dtype='uint32')\n",
    " \n",
    "for d, doc in enumerate(whole_text):\n",
    "    denom_3[d, :] *= np.sum(n_j, 0) - n_j[d] + np.sum(alpha, 1)\n",
    "    nume_3[d] *= np.sum(n_jk, 0) - n_jk[d] + alpha\n",
    " \n",
    "    \n",
    "    for s in range(N_story):\n",
    "        N_j[d, s] = s_t[s] - 1 if s_indicator[d] == s else s_t[s]\n",
    "        \n",
    "    \n",
    "        denom_4[d] *= np.sum(n_jk, 0) - n_jk[d] + np.sum(beta[0][0])\n",
    "        \n",
    "    #print(denom_4[d])\n",
    "    \n",
    "    for t in range(N_topic):\n",
    "        for v in range(N_Voca):\n",
    "        \n",
    "            fourth_term[d, :, t, v] = nume_4[d, :, t, v]/denom_4[d, :, t]\n",
    "        fourth_term_prod[d] = np.sum(np.log10(fourth_term[d]), 2)\n",
    "    \n",
    "    for t in range(N_topic):\n",
    "        third_term[d, :, t] = nume_3[d, :, t] / denom_3[d] \n",
    "        print(fourth_term_prod[d, :, t], third_term[d, :, t])\n",
    "        \n",
    "    log_third_term[d] =np.sum(np.log10(third_term[d]+fourth_term_prod[d]), 1)  \n",
    "    \n",
    "    \n",
    "    first_term = (N_j + gamma) / ((N_docs - 1) + N_story * gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in range(T_marker):\n",
    "    pi[t] = np.random.dirichlet(gamma) # 시점 T에서 문서에 할당할 Story indicator의 분포\n",
    "    for s in range(N_story):\n",
    "        theta[t, s] = np.random.dirichlet(alpha[s, :])\n",
    "        omega[t, s] = np.random.dirichlet(eps)\n",
    "        for z in range(N_topic):\n",
    "            phi[t, s, z] = np.random.dirichlet(beta[0, 0, :])\n",
    "    \n",
    "    z_agg = []\n",
    "    w_agg = []\n",
    "    std_agg_by_d = []\n",
    "    for d_id, doc in enumerate(whole_text): #print(np.random.multinomial(1, pi[time_tick, :]).argmax())        \n",
    "        N_token[d_id] = len(doc) # 문서 당 길이\n",
    "        z_d = np.zeros([N_token[d_id]], dtype=np.uint32)\n",
    "        w_d = np.zeros([N_token[d_id]], dtype=np.uint32)\n",
    "        #s_indicator[d_id] = np.random.multinomial(1, pi[t]).argmax()\n",
    "        s_indicator[d_id] = np.random.randint(N_story)\n",
    "        #print(int(s_indicator[d_id]))\n",
    "        std_agg = []\n",
    "        for w_pos, token in enumerate(doc):\n",
    "#            z_d[w_pos] = np.random.multinomial(1, theta[t, int(s_indicator[d_id]), :]).argmax()\n",
    "            z_d[w_pos] = np.random.randint(N_topic)\n",
    "            w_d[w_pos] = vocab[token]\n",
    "            #print(z_d[w_pos], w_d[w_pos])\n",
    "            n_jk[d_id, int(s_indicator[d_id]), int(z_d[w_pos])] += 1\n",
    "            n_jkv[d_id, int(s_indicator[d_id]), int(z_d[w_pos]), int(w_d[w_pos])] += 1\n",
    "            story_topic_word_d = (s_indicator[d_id], z_d[w_pos], w_d[w_pos])\n",
    "            std_agg.append(story_topic_word_d)\n",
    "        z_agg.append(z_d)\n",
    "        w_agg.append(w_d)\n",
    "        std_agg_by_d.append(std_agg)\n",
    "        n_j[d_id, int(s_indicator[d_id])] += N_token[d_id]\n",
    "        \n",
    "        \n",
    "    for s_id in s_indicator:\n",
    "        s_t[int(s_id)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(t=1):\n",
    "    pi[t] = np.random.dirichlet(gamma) # 시점 T에서 문서에 할당할 Story indicator의 분포\n",
    "    for s in range(N_story):\n",
    "        theta[t, s] = np.random.dirichlet(alpha[0, :])\n",
    "        omega[t, s] = np.random.dirichlet(eps)\n",
    "        for z in range(N_topic):\n",
    "            phi[t, s, z] = np.random.dirichlet(beta[0, 0, :])\n",
    "\n",
    "    z_agg = []\n",
    "    w_agg = []\n",
    "    std_agg_by_d = []\n",
    "    for d_id, doc in enumerate(whole_text): #print(np.random.multinomial(1, pi[time_tick, :]).argmax())        \n",
    "        N_token[d_id] = len(doc) # 문서 당 길이\n",
    "        z_d = np.zeros([N_token[d_id]], dtype=np.uint32)\n",
    "        w_d = np.zeros([N_token[d_id]], dtype=np.uint32)\n",
    "        s_indicator[d_id] = int(np.random.multinomial(1, pi[t]).argmax())\n",
    "        std_agg = []\n",
    "        for w_pos, token in enumerate(doc):\n",
    "            z_d[w_pos] = np.random.multinomial(1, theta[t, s, :]).argmax()\n",
    "            w_d[w_pos] = vocab[token]\n",
    "            print(z_d[w_pos], w_d[w_pos])\n",
    "            n_jk[d_id, int(s_indicator[d_id]), int(z_d[w_pos])] += 1\n",
    "            n_jkv[d_id, int(s_indicator[d_id]), int(z_d[w_pos]), int(w_d[w_pos])] += 1\n",
    "            story_topic_word_d = (s_indicator[d_id], z_d[w_pos], w_d[w_pos])\n",
    "            std_agg.append(story_topic_word_d)\n",
    "        z_agg.append(z_d)\n",
    "        w_agg.append(w_d)\n",
    "        std_agg_by_d.append(std_agg)\n",
    "        n_j[d_id, int(s_indicator[d_id])] += N_token[d_id]\n",
    "\n",
    "\n",
    "    for s_id in s_indicator:\n",
    "        s_t[int(s_id)] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
